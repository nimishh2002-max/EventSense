1. The "High Risk" Stress Test

"Midnight Electronic Music Rave on the Library Rooftop with 500 people and fireworks" (This should trigger high risk scores for noise, crowd control, and fire safety SOP violations.)

2. The "Logistics Heavy" Scenario

"Annual Computer Science Hackathon with 300 students staying overnight in the Student Center, serving pizza and energy drinks" (This should trigger WiFi bandwidth warnings and food safety checks.)

3. The "Weather Dependent" Event

"Spring Carnival on the South Lawn with food trucks and a live band" (This should trigger the "Rain Plan" SOP and lawn protection rules.)

4. The "Safe/Standard" Event
"Weekly Chess Club meeting in Room 304 with 15 members" (This should result in a low risk score and minimal warnings.)

In this sentences, we can see that 4 types of sentences.The sentences displayed in the double quotes are the input to the system.Only give this these sentences as input.
################
## Live Deployment Note

This project uses Ollama for on-device LLM inference (edge AI).  
Streamlit Cloud does not support running local LLM models, so the hosted UI demonstrates the frontend only.  

The full working system is shown in the demo video and can be run locally or on a self-hosted server.


---

## ‚öôÔ∏è Tech Stack  

- **LLM:** Llama 3.2 via Ollama  
- **Embeddings:** Nomic / BGE / MiniLM  
- **RAG Framework:** FAISS / ChromaDB  
- **Frontend:** Streamlit  
- **Backend:** Python  
- **Deployment:** Streamlit Cloud (UI) + Local/Server Inference  

---

## ‚ú® Key Features  

- üîç Retrieval-Augmented Generation (RAG) for accurate responses  
- üß† On-device AI inference using Ollama (no cloud dependency)  
- üìÑ Custom document ingestion (PDF / TXT / CSV)  
- ‚ö° Semantic search with vector embeddings  
- üñ•Ô∏è Interactive Streamlit chat interface  
- üîí Privacy-first AI architecture  

---

## üöÄ Live Demo  

üîó **Streamlit UI:**  
[PASTE YOUR STREAMLIT LINK HERE]

üé• **Demo Video:**  
[PASTE YOUR VIDEO LINK HERE]

---

## ‚ö†Ô∏è Deployment Note (Important)  

This project uses **Ollama for local LLM inference** to ensure privacy and offline capability.  
Streamlit Cloud does not support running local LLM models, so the hosted link demonstrates the **frontend interface only**.  

The **full working AI system is shown in the demo video** and can be executed locally or on a self-hosted server (AWS EC2 / local machine).  

---

## üõ†Ô∏è How to Run Locally  

### 1Ô∏è‚É£ Install Ollama  

```bash
https://ollama.com
